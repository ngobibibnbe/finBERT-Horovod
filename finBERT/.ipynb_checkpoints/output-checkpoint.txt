2021-01-25 21:17:21.021241: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-01-25 21:17:21.021352: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
[1]<stderr>:2021-01-25 21:18:12.333431: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
[0]<stderr>:2021-01-25 21:18:12.333614: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
[0]<stderr>:2021-01-25 21:18:12.333666: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
[1]<stderr>:2021-01-25 21:18:12.333482: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
[1]<stderr>:/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
[0]<stderr>:/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
[1]<stderr>:  return torch._C._cuda_getDeviceCount() > 0
[0]<stderr>:  return torch._C._cuda_getDeviceCount() > 0
[1]<stdout>:--------------------- /home/anne_ngobibinbe/finbert/data/sentiment_data
[0]<stdout>:--------------------- /home/anne_ngobibinbe/finbert/data/sentiment_data
[5]<stdout>:--------------------- /home/anne_ngobibinbe/finbert/data/sentiment_data
[3]<stdout>:--------------------- /home/anne_ngobibinbe/finbert/data/sentiment_data
[3]<stderr>:good_finbert_training.py:82: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.
[3]<stderr>:  pd.set_option("max_colwidth", -1)
[2]<stdout>:--------------------- /home/anne_ngobibinbe/finbert/data/sentiment_data
[4]<stdout>:--------------------- /home/anne_ngobibinbe/finbert/data/sentiment_data
[4]<stderr>:good_finbert_training.py:82: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.
[4]<stderr>:  pd.set_option("max_colwidth", -1)
[0]<stderr>:Some weights of the model checkpoint at /home/anne_ngobibinbe/finbert/models/language_model/finbertTRC2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
[0]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[0]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[0]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/anne_ngobibinbe/finbert/models/language_model/finbertTRC2 and are newly initialized: ['classifier.weight', 'classifier.bias']
[0]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[1]<stderr>:Some weights of the model checkpoint at /home/anne_ngobibinbe/finbert/models/language_model/finbertTRC2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
[1]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[1]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[1]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/anne_ngobibinbe/finbert/models/language_model/finbertTRC2 and are newly initialized: ['classifier.weight', 'classifier.bias']
[1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[5]<stderr>:Some weights of the model checkpoint at /home/anne_ngobibinbe/finbert/models/language_model/finbertTRC2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
[5]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[5]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[5]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/anne_ngobibinbe/finbert/models/language_model/finbertTRC2 and are newly initialized: ['classifier.weight', 'classifier.bias']
[5]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[3]<stderr>:Some weights of the model checkpoint at /home/anne_ngobibinbe/finbert/models/language_model/finbertTRC2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
[3]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[3]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[3]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/anne_ngobibinbe/finbert/models/language_model/finbertTRC2 and are newly initialized: ['classifier.weight', 'classifier.bias']
[3]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2]<stderr>:Some weights of the model checkpoint at /home/anne_ngobibinbe/finbert/models/language_model/finbertTRC2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
[2]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[2]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/anne_ngobibinbe/finbert/models/language_model/finbertTRC2 and are newly initialized: ['classifier.weight', 'classifier.bias']
[2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[4]<stderr>:Some weights of the model checkpoint at /home/anne_ngobibinbe/finbert/models/language_model/finbertTRC2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
[4]<stderr>:- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[4]<stderr>:- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[4]<stderr>:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/anne_ngobibinbe/finbert/models/language_model/finbertTRC2 and are newly initialized: ['classifier.weight', 'classifier.bias']
[4]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### We are in init finbert function
[2]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### Number of CPU: 6
[5]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### We are in init finbert function
[5]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### Number of CPU: 6
[3]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### We are in init finbert function
[3]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### Number of CPU: 6
[4]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### We are in init finbert function
[4]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### Number of CPU: 6
[1]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### We are in init finbert function
[1]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### Number of CPU: 6
[1]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### We are in prepare_model finbert function
[0]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### We are in init finbert function
[1]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   device: cpu n_gpu: 0, distributed training: True, 16-bits training: False
[0]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### Number of CPU: 6
[0]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### We are in prepare_model finbert function
[0]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   device: cpu n_gpu: 0, distributed training: True, 16-bits training: False
[4]<stderr>:/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
[4]<stderr>:  return torch._C._cuda_getDeviceCount() > 0
[4]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### We are in prepare_model finbert function
[4]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   device: cpu n_gpu: 0, distributed training: True, 16-bits training: False
[5]<stderr>:/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
[5]<stderr>:  return torch._C._cuda_getDeviceCount() > 0
[5]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### We are in prepare_model finbert function
[5]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   device: cpu n_gpu: 0, distributed training: True, 16-bits training: False
[3]<stderr>:/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
[3]<stderr>:  return torch._C._cuda_getDeviceCount() > 0
[3]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### We are in prepare_model finbert function
[2]<stderr>:/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
[2]<stderr>:  return torch._C._cuda_getDeviceCount() > 0
[2]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   ##### We are in prepare_model finbert function
[3]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   device: cpu n_gpu: 0, distributed training: True, 16-bits training: False
[2]<stderr>:01/25/2021 21:18:33 - INFO - finbert.finbert -   device: cpu n_gpu: 0, distributed training: True, 16-bits training: False
[0]<stdout>:we skept the folder creation step remove the file
[1]<stdout>:we skept the folder creation step remove the file
[5]<stdout>:we skept the folder creation step remove the file
[4]<stdout>:we skept the folder creation step remove the file
[2]<stdout>:we skept the folder creation step remove the file
[3]<stdout>:we skept the folder creation step remove the file
[4]<stdout>:################End of the preparation of the model
[4]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### test_auto tokenize PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})
[4]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[0]<stdout>:################End of the preparation of the model
[0]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### test_auto tokenize PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})
[0]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[1]<stdout>:################End of the preparation of the model
[1]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### test_auto tokenize PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})
[1]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[2]<stdout>:################End of the preparation of the model
[2]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### test_auto tokenize PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})
[2]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[4]<stdout>:##########################end of getting data
[3]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### test_auto tokenize PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})
[3]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[3]<stdout>:################End of the preparation of the model
[5]<stdout>:################End of the preparation of the model
[5]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### test_auto tokenize PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})
[5]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[4]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### Horovod normalement on devrait modifier le learning rate mais je ne l'ai pas fait
[4]<stderr>:01/25/2021 21:18:34 - INFO - root -   ##### je suis dans le discriminate finetuning
[4]<stderr>:01/25/2021 21:18:34 - INFO - root -   ##### HOROVOD on vient de distribuer l'optimiser
[1]<stdout>:##########################end of getting data
[0]<stdout>:##########################end of getting data
[2]<stdout>:##########################end of getting data
[1]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### Horovod normalement on devrait modifier le learning rate mais je ne l'ai pas fait
[0]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### Horovod normalement on devrait modifier le learning rate mais je ne l'ai pas fait
[0]<stderr>:01/25/2021 21:18:34 - INFO - root -   ##### je suis dans le discriminate finetuning
[1]<stderr>:01/25/2021 21:18:34 - INFO - root -   ##### je suis dans le discriminate finetuning
[2]<stderr>:01/25/2021 21:18:34 - INFO - finbert.finbert -   ##### Horovod normalement on devrait modifier le learning rate mais je ne l'ai pas fait
[2]<stderr>:01/25/2021 21:18:34 - INFO - root -   ##### je suis dans le discriminate finetuning
[0]<stderr>:01/25/2021 21:18:34 - INFO - root -   ##### HOROVOD on vient de distribuer l'optimiser
[1]<stderr>:01/25/2021 21:18:34 - INFO - root -   ##### HOROVOD on vient de distribuer l'optimiser
[2]<stderr>:01/25/2021 21:18:34 - INFO - root -   ##### HOROVOD on vient de distribuer l'optimiser
[5]<stdout>:##########################end of getting data
[5]<stderr>:01/25/2021 21:18:35 - INFO - finbert.finbert -   ##### Horovod normalement on devrait modifier le learning rate mais je ne l'ai pas fait
[5]<stderr>:01/25/2021 21:18:35 - INFO - root -   ##### je suis dans le discriminate finetuning
[5]<stderr>:01/25/2021 21:18:35 - INFO - root -   ##### HOROVOD on vient de distribuer l'optimiser
[3]<stdout>:##########################end of getting data
[3]<stderr>:01/25/2021 21:18:35 - INFO - finbert.finbert -   ##### Horovod normalement on devrait modifier le learning rate mais je ne l'ai pas fait
[3]<stderr>:01/25/2021 21:18:35 - INFO - root -   ##### je suis dans le discriminate finetuning
[3]<stderr>:01/25/2021 21:18:35 - INFO - root -   ##### HOROVOD on vient de distribuer l'optimiser
[2]<stderr>:01/25/2021 21:18:43 - INFO - root -   ######################################End of the model creation
[2]<stderr>:01/25/2021 21:18:43 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[4]<stderr>:01/25/2021 21:18:43 - INFO - root -   ######################################End of the model creation
[4]<stderr>:01/25/2021 21:18:43 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[4]<stdout>:##########################end of getting data
[4]<stdout>:##########################end of getting data
[4]<stderr>:01/25/2021 21:18:43 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[4]<stderr>:01/25/2021 21:18:43 - INFO - root -   ##### We are in get_loader finbert function
[3]<stderr>:01/25/2021 21:18:43 - INFO - root -   ######################################End of the model creation
[3]<stderr>:01/25/2021 21:18:43 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[1]<stderr>:01/25/2021 21:18:43 - INFO - root -   ######################################End of the model creation
[1]<stderr>:01/25/2021 21:18:43 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[4]<stderr>:01/25/2021 21:18:43 - INFO - finbert.utils -   *** Example ***
[4]<stderr>:01/25/2021 21:18:43 - INFO - finbert.utils -   guid: validation-1
[4]<stderr>:01/25/2021 21:18:43 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[4]<stderr>:01/25/2021 21:18:43 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[4]<stderr>:01/25/2021 21:18:43 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[4]<stderr>:01/25/2021 21:18:43 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[4]<stderr>:01/25/2021 21:18:43 - INFO - finbert.utils -   label: neutral (id = 2)
[3]<stdout>:##########################end of getting data
[3]<stderr>:01/25/2021 21:18:43 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[1]<stderr>:01/25/2021 21:18:43 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[1]<stdout>:##########################end of getting data
[3]<stdout>:##########################end of getting data
[3]<stderr>:01/25/2021 21:18:43 - INFO - root -   ##### We are in get_loader finbert function
[1]<stdout>:##########################end of getting data
[1]<stderr>:01/25/2021 21:18:43 - INFO - root -   ##### We are in get_loader finbert function
[0]<stderr>:01/25/2021 21:18:43 - INFO - root -   ######################################End of the model creation
[2]<stdout>:##########################end of getting data
[0]<stderr>:01/25/2021 21:18:43 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[2]<stderr>:01/25/2021 21:18:43 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[2]<stdout>:##########################end of getting data
[2]<stderr>:01/25/2021 21:18:43 - INFO - root -   ##### We are in get_loader finbert function
[0]<stdout>:##########################end of getting data
[0]<stderr>:01/25/2021 21:18:43 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[0]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[0]<stdout>:##########################end of getting data
[5]<stderr>:01/25/2021 21:18:44 - INFO - root -   ######################################End of the model creation
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[5]<stdout>:##########################end of getting data
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[5]<stdout>:##########################end of getting data
[5]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[4]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### HOROVOD
[4]<stderr>:01/25/2021 21:18:44 - INFO - root -   on a distribué les données d'entrainement
[4]<stdout>:
[4]<stdout>:
[4]<stdout>:
[4]<stdout>:
[4]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[4]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[2]<stdout>:
[2]<stdout>:
[2]<stdout>:
[2]<stdout>:
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[2]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### HOROVOD
[2]<stderr>:01/25/2021 21:18:44 - INFO - root -   on a distribué les données d'entrainement
[2]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[2]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[3]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### HOROVOD
[3]<stderr>:01/25/2021 21:18:44 - INFO - root -   on a distribué les données d'entrainement
[3]<stdout>:
[3]<stdout>:
[3]<stdout>:
[3]<stdout>:
[3]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[3]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[0]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### HOROVOD
[0]<stderr>:01/25/2021 21:18:44 - INFO - root -   on a distribué les données d'entrainement
[0]<stdout>:
[0]<stdout>:
[0]<stdout>:
[0]<stdout>:
[0]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[0]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[1]<stdout>:
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[1]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### HOROVOD
[1]<stderr>:01/25/2021 21:18:44 - INFO - root -   on a distribué les données d'entrainement
[1]<stdout>:
[1]<stdout>:
[1]<stdout>:
[1]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[1]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[5]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### HOROVOD
[5]<stderr>:01/25/2021 21:18:44 - INFO - root -   on a distribué les données d'entrainement
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[5]<stdout>:
[5]<stdout>:
[5]<stdout>:
[5]<stdout>:
[5]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[5]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   NOUS SOMMES DANS L'EVALUATION- HOROVOD
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   on a distribué les données d'evaluation
[4]<stdout>:
[4]<stdout>:
[4]<stdout>:
[4]<stdout>:
[4]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[4]<stdout>:
[4]<stdout>:
[4]<stdout>:
[4]<stdout>:
[4]<stderr>:01/25/2021 21:18:44 - INFO - root -   ####################################!!!!!!!!!!!!!!!!!!!!!!!Loading data
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[4]<stdout>:##########################end of getting data
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[4]<stdout>:##########################end of getting data
[4]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   NOUS SOMMES DANS L'EVALUATION- HOROVOD
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   on a distribué les données d'evaluation
[2]<stdout>:
[2]<stdout>:
[2]<stdout>:
[2]<stdout>:
[2]<stdout>:
[2]<stdout>:
[2]<stdout>:
[2]<stdout>:
[2]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[2]<stderr>:01/25/2021 21:18:44 - INFO - root -   ####################################!!!!!!!!!!!!!!!!!!!!!!!Loading data
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[2]<stdout>:##########################end of getting data
[2]<stdout>:##########################end of getting data
[2]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   NOUS SOMMES DANS L'EVALUATION- HOROVOD
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   on a distribué les données d'evaluation
[0]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[0]<stderr>:01/25/2021 21:18:44 - INFO - root -   ####################################!!!!!!!!!!!!!!!!!!!!!!!Loading data
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[0]<stdout>:
[0]<stdout>:
[0]<stdout>:
[0]<stdout>:
[0]<stdout>:
[0]<stdout>:
[0]<stdout>:
[0]<stdout>:
[0]<stdout>:##########################end of getting data
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[0]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[0]<stdout>:##########################end of getting data
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   NOUS SOMMES DANS L'EVALUATION- HOROVOD
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   on a distribué les données d'evaluation
[3]<stdout>:
[3]<stdout>:
[3]<stdout>:
[3]<stdout>:
[3]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[3]<stdout>:
[3]<stdout>:
[3]<stdout>:
[3]<stdout>:
[3]<stderr>:01/25/2021 21:18:44 - INFO - root -   ####################################!!!!!!!!!!!!!!!!!!!!!!!Loading data
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[3]<stdout>:##########################end of getting data
[3]<stdout>:##########################end of getting data
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[3]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   NOUS SOMMES DANS L'EVALUATION- HOROVOD
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   on a distribué les données d'evaluation
[5]<stdout>:
[5]<stdout>:
[5]<stdout>:
[5]<stdout>:
[5]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[5]<stdout>:
[5]<stdout>:
[5]<stdout>:
[5]<stdout>:
[5]<stderr>:01/25/2021 21:18:44 - INFO - root -   ####################################!!!!!!!!!!!!!!!!!!!!!!!Loading data
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[5]<stdout>:##########################end of getting data
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[5]<stdout>:##########################end of getting data
[5]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[2]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### HOROVOD
[2]<stderr>:01/25/2021 21:18:44 - INFO - root -   on a distribué les données d'entrainement
[2]<stdout>:
[2]<stdout>:
[2]<stdout>:
[2]<stdout>:
[2]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[2]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[4]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### HOROVOD
[4]<stderr>:01/25/2021 21:18:44 - INFO - root -   on a distribué les données d'entrainement
[4]<stdout>:
[4]<stdout>:
[4]<stdout>:
[4]<stdout>:
[4]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[4]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[0]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### HOROVOD
[0]<stderr>:01/25/2021 21:18:44 - INFO - root -   on a distribué les données d'entrainement
[0]<stdout>:
[0]<stdout>:
[0]<stdout>:
[0]<stdout>:
[0]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[0]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   NOUS SOMMES DANS L'EVALUATION- HOROVOD
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   on a distribué les données d'evaluation
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[1]<stdout>:
[1]<stdout>:
[1]<stdout>:
[1]<stdout>:
[1]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[1]<stdout>:
[1]<stdout>:
[1]<stdout>:
[1]<stdout>:
[1]<stderr>:01/25/2021 21:18:44 - INFO - root -   ####################################!!!!!!!!!!!!!!!!!!!!!!!Loading data
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[1]<stdout>:##########################end of getting data
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in get_data finbert function
[1]<stdout>:##########################end of getting data
[1]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[3]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### HOROVOD
[3]<stderr>:01/25/2021 21:18:44 - INFO - root -   on a distribué les données d'entrainement
[3]<stdout>:
[3]<stdout>:
[3]<stdout>:
[3]<stdout>:
[3]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[3]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[5]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### HOROVOD
[5]<stderr>:01/25/2021 21:18:44 - INFO - root -   on a distribué les données d'entrainement
[5]<stdout>:
[5]<stdout>:
[5]<stdout>:
[5]<stdout>:
[5]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[5]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   NOUS SOMMES DANS L'EVALUATION- HOROVOD
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   on a distribué les données d'evaluation
[2]<stdout>:
[2]<stdout>:
[2]<stdout>:
[2]<stdout>:
[2]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in train finbert function
[2]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### step_number: 5
[2]<stderr>:[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   NOUS SOMMES DANS L'EVALUATION- HOROVOD
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   on a distribué les données d'evaluation
[0]<stdout>:
[0]<stdout>:
[0]<stdout>:
[0]<stdout>:
[0]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in train finbert function
[0]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### step_number: 5
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   NOUS SOMMES DANS L'EVALUATION- HOROVOD
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   on a distribué les données d'evaluation
[4]<stdout>:
[4]<stdout>:
[4]<stdout>:
[4]<stdout>:
[4]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[0]<stderr>:[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in train finbert function
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### step_number: 5
[4]<stderr>:[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   NOUS SOMMES DANS L'EVALUATION- HOROVOD
[3]<stdout>:
[3]<stdout>:
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   on a distribué les données d'evaluation
[3]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in train finbert function
[3]<stdout>:
[3]<stdout>:
[3]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### step_number: 5
[3]<stderr>:[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[1]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### HOROVOD
[1]<stderr>:01/25/2021 21:18:44 - INFO - root -   on a distribué les données d'entrainement
[1]<stdout>:
[1]<stdout>:
[1]<stdout>:
[1]<stdout>:
[1]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[1]<stderr>:01/25/2021 21:18:44 - INFO - root -   ##### We are in get_loader finbert function
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   *** Example ***
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   guid: validation-1
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.utils -   label: neutral (id = 2)
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   NOUS SOMMES DANS L'EVALUATION- HOROVOD
[5]<stdout>:
[5]<stdout>:
[5]<stdout>:
[5]<stdout>:
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   on a distribué les données d'evaluation
[5]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in train finbert function
[5]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### step_number: 5
[5]<stderr>:[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ***** Loading data *****
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num examples = 388
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Batch size = 16
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -     Num steps = 9600
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   NOUS SOMMES DANS L'EVALUATION- HOROVOD
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   on a distribué les données d'evaluation
[1]<stdout>:
[1]<stderr>:01/25/2021 21:18:44 - INFO - root -   ################end of the getLoader
[1]<stdout>:
[1]<stdout>:
[1]<stdout>:
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### We are in train finbert function
[1]<stderr>:01/25/2021 21:18:44 - INFO - finbert.finbert -   ##### step_number: 5
[1]<stderr>:[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:we are in the unfreeze--------------
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:we are in the unfreeze--------------
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:we are in the unfreeze--------------
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:we are in the unfreeze--------------
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:we are in the unfreeze--------------
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:we are in the unfreeze--------------
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   0%|          | 0/400 [00:00<?, ?it/s][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   0%|          | 0/400 [00:00<?, ?it/s][5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   0%|          | 0/400 [00:00<?, ?it/s][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   0%|          | 0/400 [00:00<?, ?it/s][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:   0%|          | 0/400 [00:00<?, ?it/s][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stderr>:Epoch:   0%|          | 0/400 [00:00<?, ?it/s][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:(------------- zero grad end
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   0%|          | 1/400 [00:49<5:31:20, 49.83s/it][2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   0%|          | 1/400 [00:49<5:31:28, 49.84s/it][5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   0%|          | 1/400 [00:49<5:31:01, 49.78s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stderr>:Epoch:   0%|          | 1/400 [00:49<5:31:44, 49.89s/it][3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stderr>:Epoch:   0%|          | 1/400 [00:50<5:32:56, 50.07s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:   0%|          | 1/400 [00:49<5:31:34, 49.86s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   0%|          | 2/400 [01:38<5:25:41, 49.10s/it][2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   0%|          | 2/400 [01:38<5:25:50, 49.12s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   0%|          | 2/400 [01:38<5:25:37, 49.09s/it][2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stderr>:Epoch:   0%|          | 2/400 [01:38<5:26:10, 49.17s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   0%|          | 2/400 [01:38<5:25:57, 49.14s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:   0%|          | 2/400 [01:39<5:30:54, 49.89s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:(------------- zero grad end
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   1%|          | 3/400 [02:26<5:20:41, 48.47s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   1%|          | 3/400 [02:26<5:20:38, 48.46s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:   1%|          | 3/400 [02:26<5:20:38, 48.46s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   1%|          | 3/400 [02:26<5:20:43, 48.47s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:   1%|          | 3/400 [02:26<5:20:34, 48.45s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:   1%|          | 3/400 [02:27<5:22:32, 48.75s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:(------------- zero grad end
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   1%|          | 4/400 [03:13<5:17:41, 48.14s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   1%|          | 4/400 [03:13<5:17:44, 48.14s/it][5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   1%|          | 4/400 [03:13<5:17:45, 48.15s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   1%|          | 4/400 [03:13<5:17:47, 48.15s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:   1%|          | 4/400 [03:13<5:17:57, 48.18s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stderr>:Epoch:   1%|          | 4/400 [03:13<5:16:42, 47.99s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   1%|▏         | 5/400 [04:02<5:18:13, 48.34s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   1%|▏         | 5/400 [04:02<5:18:16, 48.35s/it][0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   1%|▏         | 5/400 [04:02<5:18:10, 48.33s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:(------------- zero grad end
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stderr>:Epoch:   1%|▏         | 5/400 [04:02<5:18:35, 48.39s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:(------------- zero grad end
[3]<stderr>:Epoch:   1%|▏         | 5/400 [04:02<5:18:14, 48.34s/it][3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stderr>:Epoch:   1%|▏         | 5/400 [04:02<5:17:26, 48.22s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   2%|▏         | 6/400 [04:51<5:18:57, 48.57s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stderr>:Epoch:   2%|▏         | 6/400 [04:51<5:18:58, 48.58s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:   2%|▏         | 6/400 [04:51<5:18:57, 48.57s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:(------------- zero grad end
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   2%|▏         | 6/400 [04:51<5:18:57, 48.57s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:   2%|▏         | 6/400 [04:51<5:18:17, 48.47s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:   2%|▏         | 6/400 [04:51<5:18:44, 48.54s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:(------------- zero grad end
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[1]<stdout>:(------------- zero grad end
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   2%|▏         | 7/400 [05:40<5:19:29, 48.78s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   2%|▏         | 7/400 [05:40<5:19:36, 48.79s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:   2%|▏         | 7/400 [05:40<5:19:33, 48.79s/it][3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   2%|▏         | 7/400 [05:40<5:19:30, 48.78s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stderr>:Epoch:   2%|▏         | 7/400 [05:41<5:20:02, 48.86s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:   2%|▏         | 7/400 [05:40<5:19:02, 48.71s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:(------------- zero grad end
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   2%|▏         | 8/400 [06:29<5:18:42, 48.78s/it][4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   2%|▏         | 8/400 [06:29<5:18:37, 48.77s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   2%|▏         | 8/400 [06:29<5:18:37, 48.77s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   2%|▏         | 8/400 [06:29<5:18:40, 48.78s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stderr>:Epoch:   2%|▏         | 8/400 [06:30<5:19:56, 48.97s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:   2%|▏         | 8/400 [06:29<5:18:36, 48.77s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   2%|▏         | 9/400 [07:17<5:16:27, 48.56s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   2%|▏         | 9/400 [07:17<5:16:24, 48.55s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stderr>:Epoch:   2%|▏         | 9/400 [07:17<5:16:26, 48.56s/it][2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   2%|▏         | 9/400 [07:17<5:16:28, 48.56s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stderr>:Epoch:   2%|▏         | 9/400 [07:18<5:16:59, 48.64s/it][0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:   2%|▏         | 9/400 [07:17<5:15:35, 48.43s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   2%|▎         | 10/400 [08:05<5:14:30, 48.39s/it][2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   2%|▎         | 10/400 [08:05<5:14:30, 48.39s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   2%|▎         | 10/400 [08:05<5:14:29, 48.38s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:   2%|▎         | 10/400 [08:05<5:14:26, 48.38s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   2%|▎         | 10/400 [08:05<5:14:30, 48.39s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:   2%|▎         | 10/400 [08:05<5:14:16, 48.35s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   3%|▎         | 11/400 [08:52<5:10:02, 47.82s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:fin de l'optimizer step
[5]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   3%|▎         | 11/400 [08:52<5:10:08, 47.84s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   3%|▎         | 11/400 [08:52<5:10:02, 47.82s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:   3%|▎         | 11/400 [08:52<5:09:26, 47.73s/it][3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   3%|▎         | 11/400 [08:52<5:10:07, 47.83s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stderr>:Epoch:   3%|▎         | 11/400 [08:52<5:10:32, 47.90s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:(------------- zero grad end
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   3%|▎         | 12/400 [09:41<5:11:51, 48.23s/it][4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   3%|▎         | 12/400 [09:41<5:11:46, 48.21s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   3%|▎         | 12/400 [09:41<5:11:43, 48.21s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   3%|▎         | 12/400 [09:41<5:11:47, 48.21s/it][0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stderr>:Epoch:   3%|▎         | 12/400 [09:41<5:11:28, 48.17s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stderr>:Epoch:   3%|▎         | 12/400 [09:41<5:11:18, 48.14s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[2]<stdout>: l'optimizer. step    
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:(------------- zero grad end
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   3%|▎         | 13/400 [10:27<5:07:14, 47.64s/it][4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   3%|▎         | 13/400 [10:27<5:07:18, 47.64s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   3%|▎         | 13/400 [10:27<5:07:14, 47.64s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stderr>:Epoch:   3%|▎         | 13/400 [10:27<5:07:13, 47.63s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   3%|▎         | 13/400 [10:27<5:07:15, 47.64s/it][3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:   3%|▎         | 13/400 [10:27<5:07:04, 47.61s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:   4%|▎         | 14/400 [11:15<5:07:47, 47.84s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   4%|▎         | 14/400 [11:16<5:08:06, 47.89s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stderr>:Epoch:   4%|▎         | 14/400 [11:15<5:08:07, 47.89s/it][4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   4%|▎         | 14/400 [11:15<5:08:08, 47.90s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   4%|▎         | 14/400 [11:16<5:08:08, 47.90s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:   4%|▎         | 14/400 [11:17<5:09:53, 48.17s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:   4%|▍         | 15/400 [12:03<5:07:17, 47.89s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   4%|▍         | 15/400 [12:04<5:07:52, 47.98s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   4%|▍         | 15/400 [12:04<5:07:49, 47.97s/it][3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   4%|▍         | 15/400 [12:04<5:07:53, 47.98s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   4%|▍         | 15/400 [12:04<5:07:49, 47.97s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stderr>:Epoch:   4%|▍         | 15/400 [12:04<5:08:09, 48.03s/it][0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   4%|▍         | 16/400 [12:51<5:06:20, 47.86s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   4%|▍         | 16/400 [12:51<5:06:25, 47.88s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:   4%|▍         | 16/400 [12:51<5:06:22, 47.87s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   4%|▍         | 16/400 [12:51<5:06:21, 47.87s/it][0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:   4%|▍         | 16/400 [12:52<5:07:10, 48.00s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stderr>:Epoch:   4%|▍         | 16/400 [12:51<5:05:57, 47.81s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:(------------- zero grad end
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stderr>:Epoch:   4%|▍         | 17/400 [13:39<5:04:35, 47.72s/it][2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   4%|▍         | 17/400 [13:39<5:04:35, 47.72s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   4%|▍         | 17/400 [13:39<5:04:35, 47.72s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   4%|▍         | 17/400 [13:39<5:04:38, 47.73s/it][1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:   4%|▍         | 17/400 [13:39<5:04:55, 47.77s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stderr>:Epoch:   4%|▍         | 17/400 [13:39<5:03:30, 47.55s/it][0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>: l'optimizer. step    
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   4%|▍         | 18/400 [14:28<5:06:57, 48.21s/it][1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   4%|▍         | 18/400 [14:28<5:06:59, 48.22s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:   4%|▍         | 18/400 [14:28<5:07:01, 48.22s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   4%|▍         | 18/400 [14:28<5:06:55, 48.21s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   4%|▍         | 18/400 [14:28<5:06:56, 48.21s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stderr>:Epoch:   4%|▍         | 18/400 [14:28<5:06:38, 48.16s/it][0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[4]<stdout>: l'optimizer. step    
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   5%|▍         | 19/400 [15:15<5:04:03, 47.88s/it][2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   5%|▍         | 19/400 [15:15<5:04:04, 47.89s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   5%|▍         | 19/400 [15:15<5:04:00, 47.88s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   5%|▍         | 19/400 [15:15<5:04:04, 47.89s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:   5%|▍         | 19/400 [15:15<5:03:54, 47.86s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:   5%|▍         | 19/400 [15:16<5:04:44, 47.99s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   5%|▌         | 20/400 [16:03<5:03:53, 47.98s/it][5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   5%|▌         | 20/400 [16:03<5:03:45, 47.96s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:   5%|▌         | 20/400 [16:03<5:03:46, 47.97s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stderr>:Epoch:   5%|▌         | 20/400 [16:04<5:04:45, 48.12s/it][3]<stdout>:(------------- zero grad end
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   5%|▌         | 20/400 [16:03<5:03:46, 47.96s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:   5%|▌         | 20/400 [16:03<5:03:54, 47.99s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   5%|▌         | 21/400 [16:52<5:05:08, 48.31s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   5%|▌         | 21/400 [16:52<5:05:12, 48.32s/it][4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   5%|▌         | 21/400 [16:52<5:05:11, 48.32s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stderr>:Epoch:   5%|▌         | 21/400 [16:53<5:04:04, 48.14s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   5%|▌         | 21/400 [16:53<5:05:16, 48.33s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:   5%|▌         | 21/400 [16:53<5:06:03, 48.45s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[3]<stdout>: l'optimizer. step    
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   6%|▌         | 22/400 [17:40<5:02:33, 48.03s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   6%|▌         | 22/400 [17:40<5:02:30, 48.02s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   6%|▌         | 22/400 [17:40<5:02:29, 48.01s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:   6%|▌         | 22/400 [17:40<5:02:48, 48.06s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   6%|▌         | 22/400 [17:40<5:02:29, 48.01s/it][1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:   6%|▌         | 22/400 [17:40<5:01:33, 47.87s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:   6%|▌         | 23/400 [18:27<5:00:04, 47.76s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   6%|▌         | 23/400 [18:27<4:59:52, 47.72s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   6%|▌         | 23/400 [18:27<4:59:48, 47.72s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:   6%|▌         | 23/400 [18:27<4:59:52, 47.73s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   6%|▌         | 23/400 [18:27<4:59:52, 47.73s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:   6%|▌         | 23/400 [18:27<4:59:27, 47.66s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[3]<stdout>: l'optimizer. step    
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   6%|▌         | 24/400 [19:13<4:56:30, 47.32s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stderr>:Epoch:   6%|▌         | 24/400 [19:13<4:56:29, 47.31s/it][3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   6%|▌         | 24/400 [19:13<4:56:27, 47.31s/it][0]<stderr>:Epoch:   6%|▌         | 24/400 [19:13<4:55:34, 47.16s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   6%|▌         | 24/400 [19:13<4:56:28, 47.31s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:   6%|▌         | 24/400 [19:14<4:58:20, 47.61s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:(------------- zero grad end
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:(------------- zero grad end
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   6%|▋         | 25/400 [20:01<4:55:58, 47.36s/it][4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   6%|▋         | 25/400 [20:00<4:55:38, 47.30s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stderr>:Epoch:   6%|▋         | 25/400 [20:01<4:56:25, 47.43s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   6%|▋         | 25/400 [20:01<4:55:40, 47.31s/it][5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:   6%|▋         | 25/400 [20:01<4:56:11, 47.39s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:   6%|▋         | 25/400 [20:01<4:55:29, 47.28s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   6%|▋         | 26/400 [20:48<4:55:54, 47.47s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   6%|▋         | 26/400 [20:48<4:55:47, 47.45s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:   6%|▋         | 26/400 [20:48<4:54:45, 47.29s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   6%|▋         | 26/400 [20:48<4:55:50, 47.46s/it][5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   6%|▋         | 26/400 [20:48<4:55:57, 47.48s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stderr>:Epoch:   6%|▋         | 26/400 [20:49<4:57:17, 47.69s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   7%|▋         | 27/400 [21:36<4:55:26, 47.52s/it][2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   7%|▋         | 27/400 [21:36<4:55:25, 47.52s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stderr>:Epoch:   7%|▋         | 27/400 [21:36<4:55:26, 47.52s/it][5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:   7%|▋         | 27/400 [21:36<4:54:59, 47.45s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   7%|▋         | 27/400 [21:36<4:55:29, 47.53s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:   7%|▋         | 27/400 [21:36<4:54:47, 47.42s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   7%|▋         | 28/400 [22:24<4:56:30, 47.82s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   7%|▋         | 28/400 [22:24<4:56:27, 47.82s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   7%|▋         | 28/400 [22:25<4:56:36, 47.84s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   7%|▋         | 28/400 [22:25<4:56:29, 47.82s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:   7%|▋         | 28/400 [22:25<4:56:47, 47.87s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:   7%|▋         | 28/400 [22:25<4:56:02, 47.75s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:(------------- zero grad end
[3]<stdout>: l'optimizer. step    
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stderr>:Epoch:   7%|▋         | 29/400 [23:13<4:56:20, 47.92s/it][0]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   7%|▋         | 29/400 [23:12<4:55:29, 47.79s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   7%|▋         | 29/400 [23:12<4:55:28, 47.79s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:   7%|▋         | 29/400 [23:12<4:55:32, 47.80s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   7%|▋         | 29/400 [23:12<4:55:27, 47.78s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:   7%|▋         | 29/400 [23:12<4:55:16, 47.75s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   8%|▊         | 30/400 [24:01<4:55:50, 47.97s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   8%|▊         | 30/400 [24:01<4:55:49, 47.97s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:   8%|▊         | 30/400 [24:01<4:54:51, 47.81s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   8%|▊         | 30/400 [24:01<4:55:48, 47.97s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   8%|▊         | 30/400 [24:01<4:55:41, 47.95s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:   8%|▊         | 30/400 [24:02<4:57:11, 48.19s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:(------------- zero grad end
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stderr>:Epoch:   8%|▊         | 31/400 [24:50<4:58:37, 48.56s/it][4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   8%|▊         | 31/400 [24:51<4:58:39, 48.56s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:   8%|▊         | 31/400 [24:51<4:58:38, 48.56s/it][3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   8%|▊         | 31/400 [24:51<4:58:36, 48.55s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:   8%|▊         | 31/400 [24:51<4:59:11, 48.65s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stderr>:Epoch:   8%|▊         | 31/400 [24:51<4:58:13, 48.49s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:   8%|▊         | 32/400 [25:39<4:55:58, 48.26s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   8%|▊         | 32/400 [25:38<4:56:48, 48.39s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   8%|▊         | 32/400 [25:39<4:56:48, 48.39s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stderr>:Epoch:   8%|▊         | 32/400 [25:39<4:56:46, 48.39s/it][5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   8%|▊         | 32/400 [25:39<4:56:54, 48.41s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stderr>:Epoch:   8%|▊         | 32/400 [25:39<4:56:54, 48.41s/it][0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stderr>:Epoch:   8%|▊         | 33/400 [26:27<4:55:26, 48.30s/it][2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   8%|▊         | 33/400 [26:27<4:55:25, 48.30s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   8%|▊         | 33/400 [26:27<4:55:24, 48.30s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   8%|▊         | 33/400 [26:27<4:55:26, 48.30s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:   8%|▊         | 33/400 [26:26<4:54:22, 48.13s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:   8%|▊         | 33/400 [26:28<4:56:55, 48.54s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:   8%|▊         | 34/400 [27:15<4:53:52, 48.18s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   8%|▊         | 34/400 [27:15<4:53:57, 48.19s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   8%|▊         | 34/400 [27:15<4:54:03, 48.21s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:   8%|▊         | 34/400 [27:15<4:54:00, 48.20s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   8%|▊         | 34/400 [27:15<4:54:01, 48.20s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:   8%|▊         | 34/400 [27:15<4:53:50, 48.17s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   9%|▉         | 35/400 [28:03<4:53:31, 48.25s/it][2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:   9%|▉         | 35/400 [28:03<4:53:28, 48.24s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   9%|▉         | 35/400 [28:03<4:53:24, 48.23s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   9%|▉         | 35/400 [28:03<4:53:30, 48.25s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stderr>:Epoch:   9%|▉         | 35/400 [28:03<4:52:37, 48.10s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:   9%|▉         | 35/400 [28:04<4:54:56, 48.48s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:   9%|▉         | 36/400 [28:49<4:48:57, 47.63s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:   9%|▉         | 36/400 [28:49<4:49:17, 47.68s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:(------------- zero grad end
[2]<stderr>:Epoch:   9%|▉         | 36/400 [28:49<4:49:17, 47.69s/it][2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:   9%|▉         | 36/400 [28:49<4:49:10, 47.67s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:   9%|▉         | 36/400 [28:49<4:49:15, 47.68s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:   9%|▉         | 36/400 [28:50<4:50:01, 47.81s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:   9%|▉         | 37/400 [29:38<4:49:50, 47.91s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:   9%|▉         | 37/400 [29:38<4:49:49, 47.90s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:   9%|▉         | 37/400 [29:38<4:49:51, 47.91s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:   9%|▉         | 37/400 [29:38<4:49:50, 47.91s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stderr>:Epoch:   9%|▉         | 37/400 [29:38<4:49:07, 47.79s/it][0]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:   9%|▉         | 37/400 [29:39<4:50:38, 48.04s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>: l'optimizer. step    
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  10%|▉         | 38/400 [30:26<4:49:26, 47.97s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:  10%|▉         | 38/400 [30:26<4:49:35, 48.00s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  10%|▉         | 38/400 [30:26<4:49:23, 47.97s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  10%|▉         | 38/400 [30:26<4:49:26, 47.97s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stderr>:Epoch:  10%|▉         | 38/400 [30:26<4:48:41, 47.85s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  10%|▉         | 38/400 [30:26<4:49:51, 48.04s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:(------------- zero grad end
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:  10%|▉         | 39/400 [31:13<4:45:49, 47.51s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  10%|▉         | 39/400 [31:13<4:46:43, 47.66s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stderr>:Epoch:  10%|▉         | 39/400 [31:13<4:46:44, 47.66s/it][4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  10%|▉         | 39/400 [31:13<4:46:47, 47.67s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:  10%|▉         | 39/400 [31:13<4:46:45, 47.66s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stderr>:Epoch:  10%|▉         | 39/400 [31:13<4:46:59, 47.70s/it][0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  10%|█         | 40/400 [31:59<4:43:15, 47.21s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  10%|█         | 40/400 [31:59<4:43:10, 47.20s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  10%|█         | 40/400 [31:59<4:43:13, 47.20s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stderr>:Epoch:  10%|█         | 40/400 [32:00<4:44:33, 47.43s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:  10%|█         | 40/400 [31:59<4:43:13, 47.20s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:  10%|█         | 40/400 [31:59<4:42:08, 47.02s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  10%|█         | 41/400 [32:47<4:44:00, 47.47s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:  10%|█         | 41/400 [32:47<4:44:02, 47.47s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  10%|█         | 41/400 [32:47<4:44:03, 47.48s/it][2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:  10%|█         | 41/400 [32:47<4:43:59, 47.46s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:  10%|█         | 41/400 [32:47<4:44:15, 47.51s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stderr>:Epoch:  10%|█         | 41/400 [32:47<4:43:17, 47.35s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>: l'optimizer. step    
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  10%|█         | 42/400 [33:36<4:44:44, 47.72s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:  10%|█         | 42/400 [33:35<4:44:32, 47.69s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  10%|█         | 42/400 [33:35<4:44:33, 47.69s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  10%|█         | 42/400 [33:35<4:44:31, 47.69s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:  10%|█         | 42/400 [33:35<4:44:30, 47.68s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:  10%|█         | 42/400 [33:35<4:44:21, 47.66s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[1]<stdout>:(------------- zero grad end
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  11%|█         | 43/400 [34:24<4:45:56, 48.06s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  11%|█         | 43/400 [34:24<4:45:50, 48.04s/it][5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  11%|█         | 43/400 [34:24<4:45:59, 48.06s/it][3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  11%|█         | 43/400 [34:24<4:45:55, 48.05s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  11%|█         | 43/400 [34:24<4:45:09, 47.93s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:  11%|█         | 43/400 [34:25<4:46:15, 48.11s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:  11%|█         | 44/400 [35:13<4:45:28, 48.11s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:  11%|█         | 44/400 [35:12<4:45:37, 48.14s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  11%|█         | 44/400 [35:12<4:45:34, 48.13s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stderr>:Epoch:  11%|█         | 44/400 [35:12<4:45:32, 48.12s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  11%|█         | 44/400 [35:12<4:45:34, 48.13s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  11%|█         | 44/400 [35:13<4:45:41, 48.15s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:(------------- zero grad end
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  11%|█▏        | 45/400 [36:01<4:46:15, 48.38s/it][2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:  11%|█▏        | 45/400 [36:01<4:45:24, 48.24s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  11%|█▏        | 45/400 [36:01<4:46:12, 48.37s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  11%|█▏        | 45/400 [36:01<4:46:12, 48.37s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  11%|█▏        | 45/400 [36:01<4:46:13, 48.38s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:  11%|█▏        | 45/400 [36:02<4:46:54, 48.49s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:(------------- zero grad end
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:  12%|█▏        | 46/400 [36:48<4:41:57, 47.79s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:  12%|█▏        | 46/400 [36:48<4:42:31, 47.89s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  12%|█▏        | 46/400 [36:48<4:42:24, 47.87s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  12%|█▏        | 46/400 [36:48<4:42:26, 47.87s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  12%|█▏        | 46/400 [36:48<4:42:25, 47.87s/it][5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  12%|█▏        | 46/400 [36:49<4:43:26, 48.04s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:(------------- zero grad end
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:  12%|█▏        | 47/400 [37:37<4:44:01, 48.28s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  12%|█▏        | 47/400 [37:38<4:44:34, 48.37s/it][2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:  12%|█▏        | 47/400 [37:38<4:44:28, 48.35s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:  12%|█▏        | 47/400 [37:38<4:44:32, 48.36s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:  12%|█▏        | 47/400 [37:38<4:44:30, 48.36s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  12%|█▏        | 47/400 [37:38<4:44:39, 48.38s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>: l'optimizer. step    
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  12%|█▏        | 48/400 [38:26<4:43:14, 48.28s/it][4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  12%|█▏        | 48/400 [38:26<4:43:14, 48.28s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  12%|█▏        | 48/400 [38:26<4:43:17, 48.29s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stderr>:Epoch:  12%|█▏        | 48/400 [38:25<4:42:53, 48.22s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  12%|█▏        | 48/400 [38:26<4:43:19, 48.29s/it][3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  12%|█▏        | 48/400 [38:27<4:44:29, 48.49s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:(------------- zero grad end
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:  12%|█▏        | 49/400 [39:14<4:42:58, 48.37s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:  12%|█▏        | 49/400 [39:14<4:42:53, 48.36s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  12%|█▏        | 49/400 [39:14<4:42:55, 48.36s/it][1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:  12%|█▏        | 49/400 [39:14<4:42:52, 48.35s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:  12%|█▏        | 49/400 [39:14<4:42:53, 48.36s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  12%|█▏        | 49/400 [39:15<4:43:00, 48.38s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:  12%|█▎        | 50/400 [40:04<4:42:48, 48.48s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  12%|█▎        | 50/400 [40:03<4:43:38, 48.63s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:  12%|█▎        | 50/400 [40:03<4:43:37, 48.62s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stderr>:Epoch:  12%|█▎        | 50/400 [40:03<4:43:37, 48.62s/it][2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  12%|█▎        | 50/400 [40:03<4:43:35, 48.62s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:  12%|█▎        | 50/400 [40:03<4:43:30, 48.60s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:(------------- zero grad end
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:  13%|█▎        | 51/400 [40:53<4:44:06, 48.84s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  13%|█▎        | 51/400 [40:52<4:42:58, 48.65s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:  13%|█▎        | 51/400 [40:52<4:43:05, 48.67s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:  13%|█▎        | 51/400 [40:52<4:42:53, 48.64s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:  13%|█▎        | 51/400 [40:52<4:42:56, 48.64s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:  13%|█▎        | 51/400 [40:52<4:41:37, 48.42s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  13%|█▎        | 52/400 [41:40<4:41:22, 48.51s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  13%|█▎        | 52/400 [41:40<4:41:23, 48.51s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  13%|█▎        | 52/400 [41:40<4:41:24, 48.52s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  13%|█▎        | 52/400 [41:40<4:41:26, 48.52s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stderr>:Epoch:  13%|█▎        | 52/400 [41:42<4:42:50, 48.76s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:  13%|█▎        | 52/400 [41:40<4:40:51, 48.42s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:(------------- zero grad end
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:  13%|█▎        | 53/400 [42:29<4:40:20, 48.47s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  13%|█▎        | 53/400 [42:29<4:40:19, 48.47s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  13%|█▎        | 53/400 [42:29<4:40:21, 48.48s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  13%|█▎        | 53/400 [42:29<4:40:16, 48.46s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  13%|█▎        | 53/400 [42:29<4:39:24, 48.31s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:  13%|█▎        | 53/400 [42:29<4:40:31, 48.50s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  14%|█▎        | 54/400 [43:15<4:36:04, 47.87s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  14%|█▎        | 54/400 [43:15<4:36:06, 47.88s/it][5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  14%|█▎        | 54/400 [43:15<4:36:05, 47.88s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stderr>:Epoch:  14%|█▎        | 54/400 [43:15<4:35:31, 47.78s/it][0]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stderr>:Epoch:  14%|█▎        | 54/400 [43:15<4:36:07, 47.88s/it][3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:  14%|█▎        | 54/400 [43:16<4:37:31, 48.13s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:(------------- zero grad end
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  14%|█▍        | 55/400 [44:03<4:35:41, 47.95s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  14%|█▍        | 55/400 [44:03<4:35:38, 47.94s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  14%|█▍        | 55/400 [44:03<4:35:43, 47.95s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:(------------- zero grad end
[3]<stderr>:Epoch:  14%|█▍        | 55/400 [44:03<4:35:44, 47.96s/it][3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stderr>:Epoch:  14%|█▍        | 55/400 [44:04<4:36:50, 48.15s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:  14%|█▍        | 55/400 [44:03<4:35:09, 47.85s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  14%|█▍        | 56/400 [44:52<4:36:06, 48.16s/it][2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  14%|█▍        | 56/400 [44:52<4:36:08, 48.16s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  14%|█▍        | 56/400 [44:52<4:36:08, 48.16s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stderr>:Epoch:  14%|█▍        | 56/400 [44:52<4:35:22, 48.03s/it][0]<stderr>:Epoch:  14%|█▍        | 56/400 [44:52<4:36:19, 48.20s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  14%|█▍        | 56/400 [44:52<4:36:06, 48.16s/it][1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:(------------- zero grad end
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  14%|█▍        | 57/400 [45:39<4:33:45, 47.89s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  14%|█▍        | 57/400 [45:39<4:33:43, 47.88s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stderr>:Epoch:  14%|█▍        | 57/400 [45:39<4:33:42, 47.88s/it][5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  14%|█▍        | 57/400 [45:39<4:33:44, 47.89s/it][3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:  14%|█▍        | 57/400 [45:39<4:33:03, 47.77s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:  14%|█▍        | 57/400 [45:39<4:33:16, 47.80s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  14%|█▍        | 58/400 [46:27<4:32:27, 47.80s/it][2]<stderr>:Epoch:  14%|█▍        | 58/400 [46:27<4:32:29, 47.80s/it][2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  14%|█▍        | 58/400 [46:27<4:32:28, 47.80s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  14%|█▍        | 58/400 [46:27<4:32:28, 47.80s/it][0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stderr>:Epoch:  14%|█▍        | 58/400 [46:28<4:33:22, 47.96s/it][0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:  14%|█▍        | 58/400 [46:27<4:32:19, 47.78s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:(------------- zero grad end
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  15%|█▍        | 59/400 [47:17<4:34:46, 48.35s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  15%|█▍        | 59/400 [47:16<4:34:47, 48.35s/it][5]<stderr>:Epoch:  15%|█▍        | 59/400 [47:16<4:34:45, 48.35s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stderr>:Epoch:  15%|█▍        | 59/400 [47:17<4:34:51, 48.36s/it][3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:  15%|█▍        | 59/400 [47:17<4:35:53, 48.55s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  15%|█▍        | 59/400 [47:17<4:34:21, 48.27s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:(------------- zero grad end
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:  15%|█▌        | 60/400 [48:06<4:35:32, 48.63s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  15%|█▌        | 60/400 [48:06<4:35:38, 48.64s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:  15%|█▌        | 60/400 [48:06<4:35:34, 48.63s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:  15%|█▌        | 60/400 [48:06<4:35:33, 48.63s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:  15%|█▌        | 60/400 [48:06<4:35:00, 48.53s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stderr>:Epoch:  15%|█▌        | 60/400 [48:06<4:35:45, 48.66s/it][0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  15%|█▌        | 61/400 [48:53<4:33:01, 48.32s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  15%|█▌        | 61/400 [48:53<4:32:58, 48.32s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:  15%|█▌        | 61/400 [48:53<4:33:00, 48.32s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  15%|█▌        | 61/400 [48:53<4:32:58, 48.32s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:  15%|█▌        | 61/400 [48:53<4:32:31, 48.24s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stderr>:Epoch:  15%|█▌        | 61/400 [48:54<4:32:51, 48.29s/it][0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:(------------- zero grad end
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:(------------- zero grad end
[5]<stdout>: l'optimizer. step    
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  16%|█▌        | 62/400 [49:42<4:31:55, 48.27s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  16%|█▌        | 62/400 [49:42<4:32:00, 48.28s/it][5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  16%|█▌        | 62/400 [49:42<4:31:59, 48.28s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  16%|█▌        | 62/400 [49:42<4:31:56, 48.27s/it][0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stderr>:Epoch:  16%|█▌        | 62/400 [49:42<4:32:13, 48.33s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:  16%|█▌        | 62/400 [49:42<4:32:07, 48.31s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  16%|█▌        | 63/400 [50:29<4:29:45, 48.03s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  16%|█▌        | 63/400 [50:29<4:29:51, 48.05s/it][3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stderr>:Epoch:  16%|█▌        | 63/400 [50:29<4:29:48, 48.04s/it][5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:  16%|█▌        | 63/400 [50:29<4:29:46, 48.03s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:  16%|█▌        | 63/400 [50:30<4:30:52, 48.23s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:  16%|█▌        | 63/400 [50:29<4:29:23, 47.96s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  16%|█▌        | 64/400 [51:17<4:29:38, 48.15s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  16%|█▌        | 64/400 [51:17<4:29:40, 48.16s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:  16%|█▌        | 64/400 [51:18<4:29:40, 48.16s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  16%|█▌        | 64/400 [51:17<4:29:41, 48.16s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stderr>:Epoch:  16%|█▌        | 64/400 [51:19<4:31:05, 48.41s/it][0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:  16%|█▌        | 64/400 [51:17<4:29:04, 48.05s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  16%|█▋        | 65/400 [52:06<4:29:55, 48.34s/it][4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  16%|█▋        | 65/400 [52:06<4:29:54, 48.34s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  16%|█▋        | 65/400 [52:06<4:30:00, 48.36s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  16%|█▋        | 65/400 [52:06<4:29:54, 48.34s/it][0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stderr>:Epoch:  16%|█▋        | 65/400 [52:07<4:29:30, 48.27s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:  16%|█▋        | 65/400 [52:07<4:30:19, 48.42s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:  16%|█▋        | 66/400 [52:56<4:30:59, 48.68s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  16%|█▋        | 66/400 [52:56<4:31:02, 48.69s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:  16%|█▋        | 66/400 [52:56<4:30:59, 48.68s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:  16%|█▋        | 66/400 [52:56<4:31:00, 48.68s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:  16%|█▋        | 66/400 [52:56<4:30:58, 48.68s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:  16%|█▋        | 66/400 [52:56<4:30:26, 48.58s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:(------------- zero grad end
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  17%|█▋        | 67/400 [53:44<4:29:23, 48.54s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:  17%|█▋        | 67/400 [53:44<4:29:21, 48.53s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  17%|█▋        | 67/400 [53:44<4:29:18, 48.52s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stderr>:Epoch:  17%|█▋        | 67/400 [53:44<4:29:21, 48.53s/it][5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stderr>:Epoch:  17%|█▋        | 67/400 [53:44<4:28:59, 48.47s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  17%|█▋        | 67/400 [53:44<4:29:10, 48.50s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:(------------- zero grad end
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[5]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stderr>:Epoch:  17%|█▋        | 68/400 [54:31<4:26:48, 48.22s/it][2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  17%|█▋        | 68/400 [54:31<4:26:48, 48.22s/it][2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:  17%|█▋        | 68/400 [54:31<4:26:52, 48.23s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stderr>:Epoch:  17%|█▋        | 68/400 [54:32<4:26:41, 48.20s/it][0]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stderr>:Epoch:  17%|█▋        | 68/400 [54:31<4:26:56, 48.24s/it][5]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:  17%|█▋        | 68/400 [54:31<4:26:19, 48.13s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[3]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[2]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[0]<stdout>:(------------- zero grad end
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  17%|█▋        | 69/400 [55:19<4:25:14, 48.08s/it][2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stderr>:Epoch:  17%|█▋        | 69/400 [55:19<4:24:47, 48.00s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stderr>:Epoch:  17%|█▋        | 69/400 [55:19<4:25:11, 48.07s/it][3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stderr>:Epoch:  17%|█▋        | 69/400 [55:19<4:25:20, 48.10s/it][0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stderr>:Epoch:  17%|█▋        | 69/400 [55:19<4:25:12, 48.07s/it][3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stderr>:Epoch:  17%|█▋        | 69/400 [55:19<4:25:06, 48.06s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:  18%|█▊        | 70/400 [56:07<4:24:22, 48.07s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  18%|█▊        | 70/400 [56:07<4:23:20, 47.88s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  18%|█▊        | 70/400 [56:07<4:22:56, 47.81s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stderr>:Epoch:  18%|█▊        | 71/400 [56:56<4:25:01, 48.33s/it][4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:  18%|█▊        | 71/400 [56:57<4:26:09, 48.54s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:  18%|█▊        | 71/400 [56:56<4:24:05, 48.16s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  18%|█▊        | 72/400 [57:44<4:24:03, 48.30s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:  18%|█▊        | 72/400 [57:45<4:24:50, 48.45s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stderr>:Epoch:  18%|█▊        | 72/400 [57:44<4:23:44, 48.25s/it][0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  18%|█▊        | 73/400 [58:32<4:22:54, 48.24s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:  18%|█▊        | 73/400 [58:33<4:23:33, 48.36s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stderr>:Epoch:  18%|█▊        | 73/400 [58:32<4:22:19, 48.13s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  18%|█▊        | 74/400 [59:22<4:23:56, 48.58s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stderr>:Epoch:  18%|█▊        | 74/400 [59:22<4:23:27, 48.49s/it][0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:  18%|█▊        | 74/400 [59:22<4:23:39, 48.53s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[1]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  19%|█▉        | 75/400 [1:00:11<4:23:56, 48.73s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:  19%|█▉        | 75/400 [1:00:11<4:23:36, 48.67s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:  19%|█▉        | 75/400 [1:00:11<4:23:55, 48.73s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  19%|█▉        | 76/400 [1:00:58<4:21:09, 48.36s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stderr>:Epoch:  19%|█▉        | 76/400 [1:00:59<4:21:22, 48.40s/it][1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  19%|█▉        | 76/400 [1:00:58<4:20:55, 48.32s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  19%|█▉        | 77/400 [1:01:47<4:20:14, 48.34s/it][1]<stderr>:Epoch:  19%|█▉        | 77/400 [1:01:46<4:19:46, 48.26s/it][1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  19%|█▉        | 77/400 [1:01:47<4:20:27, 48.38s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stderr>:Epoch:  20%|█▉        | 78/400 [1:02:34<4:18:51, 48.23s/it][4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stderr>:Epoch:  20%|█▉        | 78/400 [1:02:35<4:19:10, 48.29s/it][0]<stdout>:(------------- zero grad end
[0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stderr>:Epoch:  20%|█▉        | 78/400 [1:02:34<4:18:24, 48.15s/it][1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[0]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Nous sommes dans le validation stage commenté ----------------
[1]<stdout>:######## nous sommes dans une nouvelle epoch 
[1]<stderr>:Epoch:  20%|█▉        | 79/400 [1:03:23<4:18:58, 48.41s/it][1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Nous sommes dans le validation stage commenté ----------------
[4]<stderr>:Epoch:  20%|█▉        | 79/400 [1:03:22<4:17:31, 48.13s/it][4]<stdout>:######## nous sommes dans une nouvelle epoch 
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stderr>:Epoch:  20%|█▉        | 79/400 [1:03:23<4:17:09, 48.07s/it][0]<stdout>:Nous sommes dans le validation stage commenté ----------------
[0]<stdout>:######## nous sommes dans une nouvelle epoch 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[1]<stdout>: l'optimizer. step    
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[0]<stdout>: l'optimizer. step    
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stdout>:fin du loss backward
[4]<stdout>:on a sauté le backward --------------
[0]<stdout>:fin du loss backward
[0]<stdout>:on a sauté le backward --------------
[4]<stdout>: l'optimizer. step    
[0]<stdout>: l'optimizer. step    
[1]<stdout>:fin du loss backward
[1]<stdout>:on a sauté le backward --------------
[1]<stdout>: l'optimizer. step    
[1]<stdout>:fin de l'optimizer step
[1]<stdout>:fin de l'enlèvement du scheduler 
[1]<stdout>:(------------- zero grad end
[1]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[1]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[1]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:fin de l'optimizer step
[4]<stdout>:fin de l'enlèvement du scheduler 
[4]<stdout>:(------------- zero grad end
[4]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[4]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[4]<stdout>:!!!!!!!!!!!!!!!!weight to device
[0]<stdout>:fin de l'optimizer step
[0]<stdout>:fin de l'enlèvement du scheduler 
[0]<stdout>:(------------- zero grad end
[0]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[0]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[0]<stdout>:!!!!!!!!!!!!!!!!weight to device
[4]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[4]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[4]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[4]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[4]<stderr>:Epoch:  20%|██        | 80/400 [1:04:10<4:15:53, 47.98s/it][4]<stderr>:Epoch:  20%|██        | 80/400 [1:04:34<4:18:19, 48.43s/it]
[4]<stderr>:Traceback (most recent call last):
[4]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/mpi_ops.py", line 131, in _allreduce_async
[4]<stderr>:    prescale_factor, postscale_factor)
[4]<stderr>:RuntimeError: Horovod has been shut down. This was caused by an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finished execution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.
[4]<stderr>:
[4]<stderr>:During handling of the above exception, another exception occurred:
[4]<stderr>:
[4]<stderr>:Traceback (most recent call last):
[4]<stderr>:  File "good_finbert_training.py", line 191, in <module>
[4]<stderr>:    trained_model = finbert.train(train_examples=train_data, model=model)
[4]<stderr>:  File "/home/anne_ngobibinbe/finbert/finBERT/finbert/finbert.py", line 621, in train
[4]<stderr>:    loss.backward()
[4]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/torch/tensor.py", line 221, in backward
[4]<stderr>:    torch.autograd.backward(self, gradient, retain_graph, create_graph)
[4]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 132, in backward
[4]<stderr>:    allow_unreachable=True)  # allow_unreachable flag
[4]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/optimizer.py", line 196, in hook
[4]<stderr>:    handle, ctx = self._allreduce_grad_async(p)
[4]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/optimizer.py", line 160, in _allreduce_grad_async
[4]<stderr>:    postscale_factor=postscale_factor)
[4]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/mpi_ops.py", line 260, in allreduce_async_
[4]<stderr>:    return _allreduce_async(tensor, tensor, name, op, prescale_factor, postscale_factor)
[4]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/mpi_ops.py", line 133, in _allreduce_async
[4]<stderr>:    raise HorovodInternalError(e)
[4]<stderr>:horovod.common.exceptions.HorovodInternalError: Horovod has been shut down. This was caused by an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finished execution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.
[1]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[1]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[1]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[1]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[1]<stderr>:Epoch:  20%|██        | 80/400 [1:04:10<4:14:50, 47.78s/it][1]<stderr>:Epoch:  20%|██        | 80/400 [1:04:35<4:18:20, 48.44s/it]
[1]<stderr>:Traceback (most recent call last):
[1]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/mpi_ops.py", line 131, in _allreduce_async
[1]<stderr>:    prescale_factor, postscale_factor)
[1]<stderr>:RuntimeError: Horovod has been shut down. This was caused by an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finished execution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.
[1]<stderr>:
[1]<stderr>:During handling of the above exception, another exception occurred:
[1]<stderr>:
[1]<stderr>:Traceback (most recent call last):
[1]<stderr>:  File "good_finbert_training.py", line 191, in <module>
[1]<stderr>:    trained_model = finbert.train(train_examples=train_data, model=model)
[1]<stderr>:  File "/home/anne_ngobibinbe/finbert/finBERT/finbert/finbert.py", line 621, in train
[1]<stderr>:    loss.backward()
[1]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/torch/tensor.py", line 221, in backward
[1]<stderr>:    torch.autograd.backward(self, gradient, retain_graph, create_graph)
[1]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 132, in backward
[1]<stderr>:    allow_unreachable=True)  # allow_unreachable flag
[1]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/optimizer.py", line 196, in hook
[1]<stderr>:    handle, ctx = self._allreduce_grad_async(p)
[1]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/optimizer.py", line 160, in _allreduce_grad_async
[1]<stderr>:    postscale_factor=postscale_factor)
[1]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/mpi_ops.py", line 260, in allreduce_async_
[1]<stderr>:    return _allreduce_async(tensor, tensor, name, op, prescale_factor, postscale_factor)
[1]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/mpi_ops.py", line 133, in _allreduce_async
[1]<stderr>:    raise HorovodInternalError(e)
[1]<stderr>:horovod.common.exceptions.HorovodInternalError: Horovod has been shut down. This was caused by an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finished execution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.
[0]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[0]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[0]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[0]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[0]<stderr>:Epoch:  20%|██        | 80/400 [1:04:11<4:16:56, 48.18s/it][0]<stderr>:Epoch:  20%|██        | 80/400 [1:04:35<4:18:23, 48.45s/it]
[0]<stderr>:Traceback (most recent call last):
[0]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/mpi_ops.py", line 131, in _allreduce_async
[0]<stderr>:    prescale_factor, postscale_factor)
[0]<stderr>:RuntimeError: Horovod has been shut down. This was caused by an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finished execution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.
[0]<stderr>:
[0]<stderr>:During handling of the above exception, another exception occurred:
[0]<stderr>:
[0]<stderr>:Traceback (most recent call last):
[0]<stderr>:  File "good_finbert_training.py", line 191, in <module>
[0]<stderr>:    trained_model = finbert.train(train_examples=train_data, model=model)
[0]<stderr>:  File "/home/anne_ngobibinbe/finbert/finBERT/finbert/finbert.py", line 621, in train
[0]<stderr>:    loss.backward()
[0]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/torch/tensor.py", line 221, in backward
[0]<stderr>:    torch.autograd.backward(self, gradient, retain_graph, create_graph)
[0]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 132, in backward
[0]<stderr>:    allow_unreachable=True)  # allow_unreachable flag
[0]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/optimizer.py", line 196, in hook
[0]<stderr>:    handle, ctx = self._allreduce_grad_async(p)
[0]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/optimizer.py", line 160, in _allreduce_grad_async
[0]<stderr>:    postscale_factor=postscale_factor)
[0]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/mpi_ops.py", line 260, in allreduce_async_
[0]<stderr>:    return _allreduce_async(tensor, tensor, name, op, prescale_factor, postscale_factor)
[0]<stderr>:  File "/home/anne_ngobibinbe/.local/lib/python3.7/site-packages/horovod/torch/mpi_ops.py", line 133, in _allreduce_async
[0]<stderr>:    raise HorovodInternalError(e)
[0]<stderr>:horovod.common.exceptions.HorovodInternalError: Horovod has been shut down. This was caused by an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finished execution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.
[3]<stdout>:fin de l'optimizer step
[3]<stderr>:Epoch:  18%|█▊        | 70/400 [56:07<4:23:22, 47.89s/it][3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stderr>:Epoch:  18%|█▊        | 71/400 [56:56<4:24:59, 48.33s/it][3]<stderr>:Epoch:  18%|█▊        | 72/400 [57:44<4:24:08, 48.32s/it]packet_write_poll: Connection to 10.128.0.3 port 22: Broken pipe[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stderr>:
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Nous sommes dans le validation stage commenté ----------------
[3]<stdout>:######## nous sommes dans une nouvelle epoch 
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
[3]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[3]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[3]<stdout>:!!!!!!!!!!!!!!!!weight to device
[3]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[3]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[3]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[3]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[3]<stdout>:fin du loss backward
[3]<stdout>:on a sauté le backward --------------
[3]<stdout>: l'optimizer. step    
[3]<stdout>:fin de l'optimizer step
[3]<stdout>:fin de l'enlèvement du scheduler 
[3]<stdout>:(------------- zero grad end
Process 3 exit with status code 255.
[5]<stdout>:fin de l'optimizer step
[5]<stderr>:Epoch:  18%|█▊        | 70/400 [56:07<4:23:18, 47.88s/it][5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stderr>:Epoch:  18%|█▊        | 71/400 [56:56<4:25:03, 48.34s/it][5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stderr>:Epoch:  18%|█▊        | 72/400 [57:44<4:24:02, 48.30s/it][5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stderr>:Epoch:  18%|█▊        | 73/400 [58:32<4:22:54, 48.24s/it]packet_write_poll: Connection to 10.128.0.5 port 22: Broken pipe[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stderr>:
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[5]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[5]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[5]<stdout>:fin du loss backward
[5]<stdout>:on a sauté le backward --------------
[5]<stdout>: l'optimizer. step    
[5]<stdout>:fin de l'optimizer step
[5]<stdout>:fin de l'enlèvement du scheduler 
[5]<stdout>:(------------- zero grad end
[5]<stdout>:Nous sommes dans le validation stage commenté ----------------
[5]<stdout>:######## nous sommes dans une nouvelle epoch 
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[5]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[5]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[5]<stdout>:!!!!!!!!!!!!!!!!weight to device
[5]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stderr>:Epoch:  18%|█▊        | 70/400 [56:07<4:23:25, 47.89s/it]Process 5 exit with status code 255.
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stderr>:Epoch:  18%|█▊        | 71/400 [56:56<4:25:01, 48.33s/it][2]<stderr>:Epoch:  18%|█▊        | 72/400 [57:44<4:24:06, 48.31s/it][2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stderr>:Epoch:  18%|█▊        | 73/400 [58:32<4:22:57, 48.25s/it]packet_write_poll: Connection to 10.128.0.2 port 22: Broken pipe[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stderr>:
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Nous sommes dans le validation stage commenté ----------------
[2]<stdout>:######## nous sommes dans une nouvelle epoch 
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze 1
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le freeze avec l'encoder
[2]<stdout>:!!!!!!!!!!!!!!!!!un truc suspect est fait avec le batch
[2]<stdout>:!!!!!!!!!!!!!!!!weight to device
[2]<stdout>:!!!!!!!!!!!!!!!!!!!!!!!!!!! fin du weight to devices 
[2]<stdout>:!!!!!!!!!!!!!!!!!on check si c'est une classification ou pas
[2]<stdout>:!!!!!!!!!!!!!!!!!on check le step d'accumulation du gradient
[2]<stdout>:!!!!!!!!!!!!!!!!!on a pas annulé le backward
[2]<stdout>:fin du loss backward
[2]<stdout>:on a sauté le backward --------------
[2]<stdout>: l'optimizer. step    
[2]<stdout>:fin de l'optimizer step
[2]<stdout>:fin de l'enlèvement du scheduler 
[2]<stdout>:(------------- zero grad end
[2]<stdout>:Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
[2]<stdout>:######## step,batch  Iteration:   0%|          | 0/5 [00:00<?, ?it/s]
